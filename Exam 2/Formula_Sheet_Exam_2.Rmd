---
title: "Formula Sheet"
author: "Jeff Nguyen"
date: "April 2, 2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, envir=.GlobalEnv)
```

# **Gauss-Markov Theorem**  
## 6th Assumptions: Stationarity  

A variable if overtime, the mean $\mu_X$, variance $\sigma_X$ and covariance $Cov(Y_t, Y_{t-5})$ stays the same.  
- If the mean changes: it's a "mean non-stationary" variable.  
- If the variance changes: it's a "variance non-stationary" variable.  

Review the regression equation:  

\begin{equation}
  \begin{split}
    Y_t =& \beta_0 + \beta_1 X_t + \epsilon_t  \\
  \end{split}
\end{equation}
  
$Y$            | $X$              | $\epsilon$       | Regression OK? | Note                         
---            | ---              | ---              | ---            | ---        
Stationary     | Stationary       | Stationary       | Yes            |
Non-Stationary | Non-Stationary   | Stationary       | Yes            | Cointegration, needs more data          
Non-Stationary | Non-Stationary   | Non-Stationary   | No             | Spurious Regression
  
# **Time Trend**
Every time series variable can be decomposed to the following trend:  
- Time  
- Seasonal  
- Cyclical  
- Autocorrelation  
- Randomness  

**Note:** Recession is usually an economic variable.  

## How to measure time trend?  

### Step 1:  
Run Regression:  

\begin{equation}
  \begin{split}
    Y =& \beta_0 + \beta_1 T + \beta_2 T^2 + \beta T^3 + \epsilon  \\
    & \text{Note: Nothing beyond} \, T^3  \\
  \end{split}
\end{equation}
  
If $T^3$ is insignificant, drop it and so on. The trend estimated equation is:  

\begin{equation}
  \begin{split}
    \hat{Y} =& \hat{\beta_0} + \hat{\beta_1} T + \hat{\beta_2} T^2 + \hat{\beta_3} T^3  \\
    Y =& \hat{Y} + \epsilon  \\
  \end{split}
\end{equation}

$\hat{\epsilon}$ represents seasonality, cyclicality, autocorrelation, and randomness.
  
### Step 2:  
Use $\hat{\epsilon_t}$ to estimate autocorrelation:  

\begin{equation}
  \begin{split}
    \hat{\epsilon_t} =& \alpha_0 + \alpha_1 \hat{\epsilon_{t-1}}  \\
  \end{split}
\end{equation}
  
## What to do when you have spurious regression, i.e. $\epsilon$ is non-stationary?
  
In this case, $\epsilon$ shows as not scattered on a scatterplot. We have to de-trend the variable.
  
Two type of trend:  
- Deterministic: when variable explicitly driven by trend (saving account balance).  
- Stochastic: random variable (S&P 500 price movement).  
  

### De-trend Deterministic Trend Variable:  
- Run a regression on time: $Y = \beta_0 + \beta_1 T + \beta_2 T^2 + \beta T^3 + \epsilon$.  
- The estimated $\hat{Y}$ is

\begin{equation}
  \begin{split}
    \hat{Y} = \hat{\beta_0} + \hat{\beta_1} T + \hat{\beta_2} T^2 + \hat{\beta_3} T^3  \\
  \end{split}
\end{equation}
  
- The equation is then $Y = \hat{Y} + \hat{\epsilon}$ .
- Take the error of regression $\hat{\epsilon}$ as "de-trended" $Y$.  

### De-trend Stochastic Trend Variable:  
- Differencing $Y$:  

\begin{equation}
  \begin{split}
    \Delta Y =& Y_t - Y_{t-1}  \\
  \end{split}
\end{equation}
  
For example, stock price model:

\begin{equation}
  \begin{split}
    Y_t =& Y_{t-1} + \epsilon_t  \\
    &\text{via inspection:}   \\
    Y_t =& Y_0 + \sum_{i=0}^t \epsilon_i  \\
    &\text{thus:}  \\
    E[Y_t] =& Y_0  \quad \text{The model is mean-stationary.}  \\
    Var[Y_t] =& t \sigma_{\epsilon}^2  \quad \text{Variance non-stationary}  \\
  \end{split}
\end{equation}
  
Note that the random-walk hypothesis is variance-non-stationary.  
  
## Removing "wrong" Nonstationarity:  
  
- If the variable is Trend Stationary (TS) and was treated as Difference Stationary (DS), the estimated $\hat{\beta}$ will be unbiased, inefficient and consistent. Use more data to fix the problem.  
  
- If the variable is DS and was treated as TS, $\hat{\beta}$ will be biased, inefficient and inconsistent.
  
# **Smoothing Technique**  
  
## Simple Moving Average (SMA)  
  
Moving Average is not  a point forecast, it's a trend forecast.  
  
## How to calculate

$Y$    | $MA(3)$                                 
---    | ---                   
$3$  |       
$9$  | $\frac{3+9+6}{3} = 6$        
$6$  | $\frac{9+6+7}{3} = 7.33$ 
$7$  | $\frac{6+7+5}{3} = 6$ 
$5$  | $\frac{7+5+2}{3} = 5.33$ 
$2$  | 
  
```{r}
library(forecast)
library(pracma)

# Code to calculate simple MA
yArray <- c(3, 9, 6, 7, 5, 4, 2)
n = 3

smaArray <- movavg(yArray, n=n, type="s")
startloc <- ceiling((n+1)/2)
startloc
smaArray[startloc+1:length(smaArray)]
```
  
## Centered Moving Average (CMA)  
  
CMA is used to de-seasonalize data.
  
$Y$  | $MA(4)$                                 
---  | ---                   
$3$  |       
$1$  |         
$0$  | $\frac{3+1+0+4}{4} = 2$   
$4$  | $\frac{1+0+4+2}{4} = 1.75$
$2$  | $\frac{0+4+2+1}{4} = 1.75$
$1$  | 
  
```{r}
library(forecast)
library(pracma)

# Code to calculate simple MA
yArray <- c(3, 1, 0, 4, 2, 1)
n = 4

smaArray <- movavg(yArray, n=n, type="s")
loc1 <- ceiling((n+1)/2)
loc2 <- ceiling((n+1)/2)
startloc <- (loc1+loc2)/2

startloc
smaArray[startloc+1:length(smaArray)]
```
  
## Weighted Moving Average (WMA)  

\begin{equation}
  \begin{split}
    Y_{t+1} =& .5 Y_t + .3 Y_{t-1} + .2 Y_{t-2}  \\
    \sum w =& .5 + .3 + .2 = 1  \\
  \end{split}
\end{equation}
  
Note: assign more weights to most recent data.  
  
```{r}
yArray <- c(3, 9, 6, 7, 5, 4, 2)
n = 3

movavg(yArray, n=n, type="w")
```
  
## Problem of Moving Average  
  
- Losing of observations (for example: beginning and end of data) and degree of freedom.  
- Weights are equal weights when in reality, more weights should be given to most recent data.  
- It's not a point forecast.  

**Remarks:  Exponential Smoothing solve these problems based on the idea that the forecast of future time period is based on:**
  
\begin{equation}
  \begin{split}
    Y_{t+1} =& Y_t^F + \alpha (Y_t - Y_t^F)  \\
    &\text{where} \, 0 < \alpha < 1 \, \text{is the damping effect}  \\
    &\text{therefore:}  \\
    Y_{t+1}^F =& \alpha Y_t + (1-\alpha) Y_t^F  \\
              =& \alpha Y_t + \alpha(1-\alpha) Y_{t-1} + \alpha (1-\alpha)^2 Y_{t-2} + ...  \\
  \end{split}
\end{equation}

# Reviewing Forecasting:  
  
ME = Mean Error (Average of errors).  
RMSE = Root Mean Square Error (SD of residuals, i.e. how far data is from regression line).  
MAE = Mean Absolute Error (Average of distance from regression line).  
MAPE = Mean Absolute Percentage Error (measure prediction accuracy).  
MASE = Mean Absolute Scaled Error (prediction accuracy).  

When decompose time series, use additive for log variables and multiplicative for others.  

## Simple Exponential Smoothing  
  
Use if data is flat.  

Pick the damping effect that yields the smallest ACF1 metric.  

### Exercise: Given damping effect $\alpha$, do one-period Exponential Smoothing forecast  

Use the formula $Y_{t+1} = Y_t^F + \alpha (Y_t - Y_t^F)$. Note that it's not possible to do more than one-period ahead forecast using ES.  

Confidence Interval:
\begin{equation}
  \begin{split}
    Y^F - 1.96 \epsilon_F \leqslant Y_{actual} \leqslant Y^F + 1.96 \epsilon_F  \
  \end{split}
\end{equation}

## Holt Trend Model  
  
Use if data has time trend use Holt Exponential Smoothing. If data has seasonality, use Holt-Winter Exponential Smoothing.

\begin{equation}
  \begin{split}
    \text{Forecast Equation:}  \\
    Y_{t+h}^F =& l_t + h b_t   \\
    \text{Level Equation:}   \\
    l_t =& \alpha y_t + (1-\alpha) (l_{t-1} + b_{t-1})  \\
    \text{where} \, \alpha \, \text{is the smoothing coefficient for level}  \\
    \text{Trend Equation:}  \\
    b_t =& \beta (l_t - l_{t-1}) + (1-\beta) b_{t-1}  \\
    \text{where} \, \beta \, \text{is smoothing coefficient for trend}  \\
  \end{split}
\end{equation}
  
## Random Walk  
  
### Random Walk (Naive Forecasting)  
  
\begin{equation}
  \begin{split}
    Y_t =& Y_0 + \sum_{i=1}^n \epsilon_i   \\
    \mu =& E[Y_t] = Y_0 \quad \Leftarrow \quad \text{stationary}   \\
    \sigma^2 =& Var[Y_t] = t \sigma_{\epsilon}^2 \quad \Leftarrow \quad \text{non-stationary}  \\
  \end{split}
\end{equation}
  
### Random Walk with Drift  
  
\begin{equation}
  \begin{split}
    Y_t =& Y_0 + t \alpha_0 + \sum_{i=1}^T \epsilon_i   \\
    \mu =& E[Y_t] = Y_0 + t \alpha_0    \quad \Leftarrow \quad \text{non-stationary}   \\
    \sigma^2 =& Var[Y_t] = t \sigma_{\epsilon}^2 \quad \Leftarrow \quad \text{non-stationary}  \\
    \text{and}  \\
    D(Y_t) =& \epsilon_t  \\
    E[DY_t] =& 0  \\
    Var[DY_t] =& \sigma_{\epsilon}^2  \\
  \end{split}
\end{equation}
  
### Random Walk with Drift and Noise  

\begin{equation}
  \begin{split}
    Y_t =& \alpha_0 + Y_{t-1} + \epsilon_t + \eta_t   \\
    E[Y_t] =& Y_0 + \alpha_0 t  \\
    Var[Y_t] =& \sigma_{\epsilon}^2 + \sigma_{\eta}^2  \\
  \end{split}
\end{equation}
  
# 1st Order Linear Homogenous DE  

\begin{equation}
  \begin{split}
    Y_t =& \alpha Y_{t-1} \quad \text{given} \, Y_0  \\
    \text{Solution:} \quad Y_t =& \alpha^t Y_0  \\
    \lim_{|\alpha| \to 1} Y_t =& 0 \Rightarrow \, \text{convergent}  \\ 
    \lim_{|\alpha| \to \infty} Y_t =& \infty \Rightarrow \, \text{divergent}  \\ 
  \end{split}
\end{equation}
  
**If $0<\alpha<1$:** Y is a non-oscillating convergent model (stationary).  
**If $\alpha>1$:** Y is a non-oscillating divergent model (non-stationary).  
**If $-1<\alpha<0$:** Y is an oscillating convergent model.  
**If $\alpha<-1$:** Y is an oscillating divergent model.  

# 1st Order Linear Non-Homogenous DE  
  
\begin{equation}
  \begin{split}
    Y_t =& \alpha_0 + \alpha_1 Y_{t-1} \quad \text{given} \, Y_0  \\
    \text{Solution:} \quad Y_t =& \alpha_0 \frac{1-\alpha_1^t}{1-\alpha_1} + \alpha_1^t Y_0  \\
  \end{split}
\end{equation}
  
This result is used in Box-Jenkins methodology and ARIMA model.
  
# ARIMA Model  
  
Note that ARIMA has "short memory" because of its recursive nature $\Rightarrow$ each iteration erodes efficiency.  

## Examples:

### $Y_t = \alpha_0 + \alpha_1 Y_{t-1} + \epsilon_t$ $\Rightarrow$ ARIMA(1,0,0) or AR(1)  
### $DY_t = \alpha_0 + \alpha_1 DY_{t-1} + \epsilon_t$ $\Rightarrow$ ARIMA(1,1,0)  
### $Y_t = \alpha_0 + \alpha_1 Y_{t-1} + \epsilon_t + \beta \epsilon_{t-1}$ $\Rightarrow$ ARIMA(1,0,1) or ARMA(1,1)  
### $DY_t = \alpha_0 + \alpha_1 DY_{t-1} + \epsilon_t + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2}$ $\Rightarrow$ ARIMA(1,1,2)  
### $DY_t = \alpha_0 + \alpha_1 Y_{t-1} + \epsilon_t+ \beta \epsilon_{t-1}$ $\Rightarrow$ Mis-specified model as $DY_{t-1}$ is missing.  

## Stationarity & Invertibility   

### Stationary if $\sum_{i=1}^n |\alpha_i| < 1$ and vice versa. Note that if $|\alpha_i| < 1$ and $\sum_{i=1}^n |\alpha_i| > 1$, non-stationarity is a possibility and an (A)DF Test is appropriate.  
### Invertibile if $\sum_{i=1}^n |\beta_i| < 1$ and N/A if there is no $\beta_i$.  

## Quick Calculations  
  
  
### AR(1) Model:  

```{r}
y_0 <- 20
sigmaSq <- .08225
alpha_0 <- 2.5
alpha_1 <- .7

f <- function(y_t) {alpha_0 + alpha_1*y_t}
y_1 <- f(y_0)
y_2 <- f(y_1)
y_3 <- f(y_2)
```
  
| Forecast Variable      | Forecast Value | 95% Confidence Interval Equation                               | 95% Confidence Interval                                                                     |
|------------------------|------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| $Y_1^F = `r alpha_0` + `r alpha_1` Y_0$ | $`r y_1`$        | $Y_{t+1}^F \pm 1.96\sqrt{\sigma^2}$                            | $P(`r y_1` - 1.96  \sqrt{`r sigmaSq`} \leqslant Y_{t+1} \leqslant `r y_1` + 1.96  \sqrt{`r sigmaSq`}) = .95$ |
| $Y_2^F = `r alpha_0` + `r alpha_1` Y_1$ | $`r y_2`$        | $Y_{t+2}^F \pm 1.96\sqrt{\sigma^2(1+\alpha^2)}$                | $P(`r y_2` - 1.96 \sqrt{`r sigmaSq`*(1+`r alpha_1`^2)} \leqslant Y_{t+2} \leqslant `r y_2` + 1.96 \sqrt{`r sigmaSq`*(1+`r alpha_1`^2)}) = .95$             |
| $Y_3^F = `r alpha_0` + `r alpha_1` Y_2$ | $`r y_3`$        | $Y_{t+3}^F \pm 1.96 \sqrt{\sigma^2(1+\alpha_1^2 + \alpha_1^4)}$ | $P(`r y_3` - 1.96 \sqrt{`r sigmaSq` (1+`r alpha_1`^2+`r alpha_1`^4)} \leqslant Y_{t+3} \leqslant `r y_3` + 1.96 \sqrt{`r sigmaSq` (1+`r alpha_1`^2+`r alpha_1`^4)}) = .95$ |
  

  
```{r}
yLR <- alpha_0 / (1 - alpha_1)
sigmaSq_yLR <- sigmaSq / (1 - alpha_1^2)
```
  
| Forecast Long Run                          | Forecast Value | 95% Confidence Interval Equation                      | 95% Confidence Interval                                              |
|--------------------------------------------|------------------|-------------------------------------------------------|----------------------------------------------------------------------|
| $Y_{LR} = \frac{`r alpha_0`}{`r alpha_1`}$ | $`r yLR`$        | $Y_{LR} \pm 1.96\sqrt{\frac{\sigma^2}{1-\alpha_1^2}}$ | $P(`r yLR` - 1.96 \sqrt{`r sigmaSq_yLR`} \leqslant Y_{LR} \leqslant `r yLR` + 1.96 \sqrt{`r sigmaSq_yLR`}) = .95$ |
  
### AR(2) Model:  

```{r}
alpha_0 <- 0
alpha_1 <- .7
alpha_2 <- .15
y_0 <- 5
y_1 <- 6
sigmaSq <- 1.2
f <- function(y_tMinus1, y_tMinus2) {alpha_0 + alpha_1*y_tMinus1 + alpha_2*y_tMinus2}
y_2 <- f(y_1, y_0)
y_3 <- f(y_2, y_1)
y_4 <- f(y_3, y_2)
```
  
| Forecast Variable      | Forecast Value | 95% Confidence Interval Equation                               | 95% Confidence Interval                                                                     |
|------------------------|------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| $Y_2^F = `r alpha_0` + `r alpha_1` Y_1 + `r alpha_2` Y_0$ | $`r y_2`$        | $Y_{t+1}^F \pm 1.96 \sqrt{\sigma^2 * (1 + \alpha_1^2)}$                            | $P(`r y_2` - 1.96 \sqrt{`r sigmaSq`*(1+`r alpha_1`^2)} \leqslant Y_{t+1} \leqslant `r y_2` + 1.96 \sqrt{`r sigmaSq` (1+`r alpha_1`^2)}) = .95$                                |
| $Y_3^F = `r alpha_0` + `r alpha_1` Y_2 + `r alpha_2` Y_1$ | $`r y_3`$        | $Y_{t+2}^F \pm 1.96\sqrt{\sigma^2(1+\alpha_1^2 + \alpha_2^2 + \alpha_1^4 + 2 \alpha_1^2 \alpha_2)}$                | $P(`r y_3` - 1.96 \sqrt{`r sigmaSq` (1+ `r alpha_1`^2 + `r alpha_2`^2 + `r alpha_1`^4 + 2 `r alpha_1`^2 `r alpha_2`)} \leqslant Y_{t+2} \leqslant `r y_3` + 1.96 \sqrt{`r sigmaSq` (1+ `r alpha_1`^2 + `r alpha_2`^2 + `r alpha_1`^4 + 2 `r alpha_1`^2 `r alpha_2`)}) = .95$             |
| $Y_4^F = `r alpha_0` + `r alpha_1` Y_3 + `r alpha_2` Y_2$ | $`r y_4`$        | $Y_{t+3}^F \pm 1.96\sqrt{\sigma^2[1+\alpha_1^2 + (\alpha_1^2 + \alpha_2)^2 + (\alpha_1^3 + 2 \alpha_2 \alpha_1)^2]}$ | $P(`r y_4` - 1.96 \sqrt{`r sigmaSq`*(1+ `r alpha_1`^2 + (`r alpha_1`^2+`r alpha_2`)^2 + (`r alpha_1`^3 + 2 `r alpha_1` `r alpha_2`)^2)} \leqslant Y_{t+3} \leqslant `r y_4` + 1.96 \sqrt{`r sigmaSq`*(1+ `r alpha_1`^2 + (`r alpha_1`^2+`r alpha_2`)^2 + (`r alpha_1`^3 + 2 `r alpha_1` `r alpha_2`)^2)}) = .95$ |
  

```{r}
yLR <- alpha_0 / (1 - alpha_1 - alpha_2)
sigmaSq_yLR <- (1 - alpha_2)*sigmaSq / ((1 + alpha_2)*((1 - alpha_2)^2 - alpha_1^2))
```
  
| Forecast Long Run                          | Forecast Value | 95% Confidence Interval Equation                      | 95% Confidence Interval                                              |
|--------------------------------------------|------------------|-------------------------------------------------------|----------------------------------------------------------------------|
| $Y_{LR} = \frac{`r alpha_0`}{1 - `r alpha_1` - `r alpha_2`}$ | $`r yLR`$        | $Y_{LR} \pm 1.96\sqrt{\frac{(1-\alpha_2)*\sigma^2}{(1+\alpha_2) [ (1-\alpha_2)^2 - \alpha_1^2 ]}}$ | $P(`r yLR` - 1.96 \sqrt{`r sigmaSq_yLR`} \leqslant Y_{LR} \leqslant `r yLR` + 1.96 \sqrt{`r sigmaSq_yLR`}) = .95$ |
  
  
### MA(1) Model:  

```{r}
alpha_0 <- 10
beta_1 <- .64  # coefficient of epsilon_{t}
beta_2 <- 0  # coeff of epsilon_{t-1}
e_1 <- 2.5
e_2 <- 0
e_3 <- 0
e_4 <- 0
sigmaSq <- 1.21
f <- function(e_t, e_tMinus1) {alpha_0 + beta_1*e_t + beta_2*e_tMinus1}
y_2 <- f(e_2, e_1)
y_3 <- f(e_3, e_2)
y_4 <- f(e_4, e_3)
```
  
| Forecast Variable      | Forecast Value | 95% Confidence Interval Equation                               | 95% Confidence Interval                                                                     |
|------------------------|------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| $Y_2^F = `r alpha_0` + `r beta_1` \epsilon_2 + `r beta_2` \epsilon_1$ | $`r y_2`$        | $Y_{t+1}^F \pm 1.96\sqrt{\sigma^2}$                            | $P(`r y_2` - 1.96  \sqrt{`r sigmaSq`} \leqslant Y_{t+1} \leqslant `r y_2` + 1.96  \sqrt{`r sigmaSq`}) = .95$ |
| $Y_3^F = `r alpha_0` + `r beta_1` \epsilon_3 + `r beta_2` \epsilon_2$ | $`r y_3`$        | $Y_{t+2}^F \pm 1.96\sqrt{\sigma^2(1+\beta_2^2)}$                | $P(`r y_3` - 1.96 \sqrt{`r sigmaSq`*(1+`r beta_2`^2)} \leqslant Y_{t+2} \leqslant `r y_2` + 1.96 \sqrt{`r sigmaSq`*(1+`r beta_2`^2)}) = .95$             |
| $Y_4^F = `r alpha_0` + `r beta_1` \epsilon_4 + `r beta_2` \epsilon_3$ | $`r y_4`$        | $Y_{t+3}^F \pm 1.96 \sqrt{\sigma^2(1+\beta_2^2 + \beta_2^4)}$ | $P(`r y_4` - 1.96 \sqrt{`r sigmaSq` (1+`r beta_2`^2+`r beta_2`^4)} \leqslant Y_{t+3} \leqslant `r y_4` + 1.96 \sqrt{`r sigmaSq` (1+`r beta_2`^2+`r beta_2`^4)}) = .95$ |
  

  
```{r}
yLR <- alpha_0
sigmaSq_yLR <- sigmaSq / (1 - beta_2^2)
```
  
| Forecast Long Run                          | Forecast Value | 95% Confidence Interval Equation                      | 95% Confidence Interval                                              |
|--------------------------------------------|------------------|-------------------------------------------------------|----------------------------------------------------------------------|
| $Y_{LR} = \alpha_0$ | $`r yLR`$        | $Y_{LR} \pm 1.96 \sqrt{\frac{\sigma^2}{1-\beta_2^2}}$ | $P(`r yLR` - 1.96 \sqrt{`r sigmaSq_yLR`} \leqslant Y_{LR} \leqslant `r yLR` + 1.96 \sqrt{`r sigmaSq_yLR`}) = .95$ |
  
### ARIMA (1,1,0) $DY_t = \alpha_0 + \alpha_1 DY_{t-1}$  
  
```{r}
alpha_0 <- 10
alpha_1 <- .2  # coefficient of DY_{t-1}
y_0 <- 8
y_1 <- 10
sigmaSq <- 5
fD <- function(y_t, y_tMinus1) {alpha_0 + alpha_1*(y_t - y_tMinus1)}
dy_2 <- fD(y_1, y_0)
y_2 <- y_1 + dy_2
```
  
| Forecast Variable      | Forecast Value | 95% Confidence Interval Equation                               | 95% Confidence Interval                                                                     |
|------------------------|------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------|
| $Y_2^F = `r alpha_0` + `r alpha_1` DY_1 + Y_1$ | $`r y_2`$        | $Y_{t+1}^F \pm 1.96\sqrt{\sigma^2}$                            | $P(`r y_2` - 1.96  \sqrt{`r sigmaSq`} \leqslant Y_{t+1} \leqslant `r y_2` + 1.96  \sqrt{`r sigmaSq`}) = .95$ |
| $DY_2^F = `r alpha_0` + `r alpha_1` DY_1$ | $`r dy_2`$        | $DY_{t+1}^F \pm 1.96\sqrt{\sigma^2}$                | $P(`r dy_2` - 1.96 \sqrt{`r sigmaSq`} \leqslant DY_{t+1} \leqslant `r dy_2` + 1.96 \sqrt{`r sigmaSq`}) = .95$             |


$Y_{LR}$ does not exists as $Y$ is non-stationary and thus, does not converges inside a confidence interval. A confidence interval does not exists.  

  

# Box-Jenkins' Methodology
  
## Objective  
Estimating the data generating process of a time-series variable as a function of time $Y_t = f(t)$.  

## Why ARIMA Model  
- When structural modes are not useful: Daily stock price.  
- Sometimes structural models are not good forecasting models.  
- Sometimes data are not available for structural models.  

## Three Stages of ARIMA:  
- **Identification:**  Finding order of $ARIMA(p,d,q)$ using correlogram.  
- **Estimation**  
- **Diagnostics**  

## Two Requirements of ARIMA:  
- **Stationarity:** The variable should be tested for stationarity. If the variable is non-stationary, it should be converted to stationary by first differencing the variable.  
- **Parsimony:** Selecting the model which is parsimonious (simple but have great explaining power) using AIC, SBC. Models which are not parsimonious may fit better to noise than signals. They will not forecast future well.  
  
## Measurement of Goodness in ARIMA:  
  
AIC and SBC (not $R^2$) are measurement of goodness of fit for ARIMA estimates, with SBC emphasizing parsimony. A lower AIC or BIC value indicates good fit.  

\begin{equation}
  \begin{split}
    AIC =& \ln{\sigma^2} + \frac{2k}{T}  \\
    SBC =& \ln{\sigma^2} + \frac{k}{T} \ln{T}  \\
    &\text{where}  \\
    k =& p + q + 1  \\
    T =& \, \text{Number of observations}  \\
  \end{split}
\end{equation}
  
**When run ARIMA model in R, to determine if coefficient is significant or not, divide coefficient by the S.E.. If result is large then it is significant.**  


## Diagnostics:  
  
ARIMA will have a good forecasting output if:  
- The residual of the regression is white noise. To test for white noise error, use correlogram of the residual.  
- Coefficients of ARIMA are stable. To test for the stability of the coefficients, split the data, run two ARIMA and do an F test.  

## F Test of Diagnostics:  
- Split data to two parts, fit ARIMA to each part and to the whole sample.  

\begin{equation}
  \begin{split}
    F =& \frac{\frac{SSR_T - SSR_1 - SSR_2}{k}}{\frac{SSR_1 + SSR_2}{T - 2k}}  \\
    H_0:& \, \text{Coefficients are the same.}  \\
    H_a:& \, \text{Coefficients are not the same.}  \\
    &\text{where}  \\
    SSR_T =& \, \text{Total Sum Squared Residual}  \\
    SSR_1 =& \, \text{SSR for first part of the sample}  \\
    SSR_2 =& \, \text{SSR for second part of the sample}  \\
    K =& p + q + 1  \\
    T =& \, \text{Sample size}  \\
  \end{split}
\end{equation}

## Properties of a Good ARIMA Model:  
- Parsimonious.  
- Has coefficients that are stationary and are invertible.  
- Fits data well.  
- Has residuals that are white noise.  
- Has coefficients that do not change over sample period.  
- Has good out-of-sample forecast.  
  
## Models of Volatility:  
Many financial time series go through periods of tranquility and periods of volatility. Volatility and its measures are some of the most important concepts in finance.  

## Historical Volatility:  
Measures the variance or standard deviation of returns over some period and uses it as measure of forecast of future volatility.  It's used in option valuation and option pricing.  

## Characteristics of Financial Data:  
- Non-stationary.  
- Leptokurtosis: sharp peaks, fat tails.  
- Volatility clustering.  
- Leverage effect: Volatility rise more with price fall than price rise.  
- Has coefficients that do not change over sample period.  
- Has plenty noises as well as signals.  
- Volatility is mean reverting.  
  
# Reading ACF and PACF  

## Non-stationary: 
- If PACF has a single spike at $r_0 \approx 1$ and the rest is insignificant and ACF mostly significant and gradually decreasing $\Rightarrow$ non-stationary model.  

## $MA(\infty)$ and $AR(\infty)$  

Several spikes in ACF (significant coefficients) $\Rightarrow MA(\infty) \Rightarrow AR(1)$.  
Several spikes in PACF (significant coefficients) $\Rightarrow AR(\infty) \Rightarrow MA(1)$.  
  
# (A)DF Test  
- Failed to reject the null hypothesis: there is a unit root $\Rightarrow$ non-stationary.  
- Reject the null hypothesis: stationary.  

# Test for White Noise  
  
## Box-Pierce Test  

**Null hypothesis: Residuals are White Noise.**  

\begin{equation}
  \begin{split}
    Q =& T \sum r^2  \\
  \end{split}
\end{equation}
  
## Box-Ljung Test  

**Null hypothesis: Residuals are White Noise.**  

\begin{equation}
  \begin{split}
    Q =& T(T+2) \sum (T-K)^{-r^2}  \\
  \end{split}
\end{equation}
  
  
Recall AR(2)  

\begin{equation}
  \begin{split}
    Y_t =& \alpha_0 + \alpha_1  Y_{t-1} + \alpha_2 Y_{t-2}  \\
    \text{Regressing} \, \epsilon \, \text{:}  \\
    b_i =& \alpha_1 b_{i-1} + \alpha_2 b_{i-2}  \\
    \text{We have:}  \\
    \epsilon_{Y_{t+1}} =& \sum_{i=0}^1 b_i \epsilon_{t-1}  \\
                       =& b_0 \epsilon_t + b_1 \epsilon_{t-1}  \\
                       =& \epsilon_t + \alpha_1 \epsilon_{t-1}  \\
    \text{Thus:}  \\
    Var[Y_{t+1}] =& (1 + \alpha_1^2) \sigma_{\epsilon}^2  \\
  \end{split}
\end{equation}
  
Similarly:  

\begin{equation}
  \begin{split}
    \epsilon_{Y_{t+2}} =& \sum_{i=0}^2 b_i \epsilon_{t-1}  \\
                       =& b_0 \epsilon_t + b_1 \epsilon_{t-1} + b_2 \epsilon_{t-2}  \\
    \text{Recall that:} \, b_i = \alpha_1 b_{i-1} + \alpha_2 b_{i-2}  \\
    \text{We have:}
    \epsilon_{Y_{t+2}} =& \epsilon_t + \alpha_1 \epsilon_{t-1} + (\alpha_1 b_1 + \alpha_2  b_0) \epsilon_{t-2}  \\
                       =& \epsilon_t + \alpha_1 \epsilon_{t-1} + (\alpha_1^2 + \alpha_2) \epsilon_{t-2}  \\
    \text{Thus:}  \\
    Var[Y_{t+2}] =& \big[ 1 + \alpha_1^2 + (\alpha_1^2 + \alpha_2)^2 \big] \sigma_{\epsilon}^2  \\
  \end{split}
\end{equation}